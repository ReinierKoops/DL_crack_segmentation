{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-82d331e0-79e4-40f0-ba43-0ad2ccc7fea5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Reproduce DL\n",
    "## Automated Pavement Crack Segmentation\n",
    "\n",
    "We start by setting up the actual Architecture. This means making sure all weights are properly initialized and all layers are connected. \n",
    "\n",
    "We make use of PyTorch for the implementation.\n",
    "\n",
    "Multiple parts come together (A U-based ResNet);\n",
    "- We recreate ResNet34 and remove the last two layers\n",
    "- We made sure that a ResNet-block is either 4 or 6 layers depending on if stride is not 1 (which in our case always happens when the in_channels are not equal to out_channels)\n",
    "- We use transfer learning such that the ResNet34 parameters are initialized as if trained on ImageNet\n",
    "- We create Squeeze and Excitation blocks that are applied per Channel (cSE) and per Spatial (sSE) (image)\n",
    "- These two blocks are combined (scSE) and then the maximum of this is taken\n",
    "- Each convolutional layer its parameters are initialized via \"He Kaiming\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00002-da764467-46fa-4b0b-8c70-b51631d8659f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10648,
    "execution_start": 1616684405017,
    "source_hash": "750d6355",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 160, 240]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 160, 240]             128\n",
      "              ReLU-3         [-1, 64, 160, 240]               0\n",
      "            Conv2d-4        [-1, 128, 160, 240]           8,320\n",
      "         MaxPool2d-5          [-1, 64, 80, 120]               0\n",
      "            Conv2d-6          [-1, 64, 80, 120]          36,864\n",
      "       BatchNorm2d-7          [-1, 64, 80, 120]             128\n",
      "              ReLU-8          [-1, 64, 80, 120]               0\n",
      "            Conv2d-9          [-1, 64, 80, 120]          36,864\n",
      "      BatchNorm2d-10          [-1, 64, 80, 120]             128\n",
      "             ReLU-11          [-1, 64, 80, 120]               0\n",
      "         ResBlock-12          [-1, 64, 80, 120]               0\n",
      "           Conv2d-13          [-1, 64, 80, 120]          36,864\n",
      "      BatchNorm2d-14          [-1, 64, 80, 120]             128\n",
      "             ReLU-15          [-1, 64, 80, 120]               0\n",
      "           Conv2d-16          [-1, 64, 80, 120]          36,864\n",
      "      BatchNorm2d-17          [-1, 64, 80, 120]             128\n",
      "             ReLU-18          [-1, 64, 80, 120]               0\n",
      "         ResBlock-19          [-1, 64, 80, 120]               0\n",
      "           Conv2d-20          [-1, 64, 80, 120]          36,864\n",
      "      BatchNorm2d-21          [-1, 64, 80, 120]             128\n",
      "             ReLU-22          [-1, 64, 80, 120]               0\n",
      "           Conv2d-23          [-1, 64, 80, 120]          36,864\n",
      "      BatchNorm2d-24          [-1, 64, 80, 120]             128\n",
      "             ReLU-25          [-1, 64, 80, 120]               0\n",
      "         ResBlock-26          [-1, 64, 80, 120]               0\n",
      "           Conv2d-27         [-1, 128, 80, 120]           8,320\n",
      "           Conv2d-28          [-1, 128, 40, 60]          73,728\n",
      "      BatchNorm2d-29          [-1, 128, 40, 60]             256\n",
      "             ReLU-30          [-1, 128, 40, 60]               0\n",
      "           Conv2d-31          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 40, 60]             256\n",
      "           Conv2d-33          [-1, 128, 40, 60]           8,192\n",
      "      BatchNorm2d-34          [-1, 128, 40, 60]             256\n",
      "             ReLU-35          [-1, 128, 40, 60]               0\n",
      "         ResBlock-36          [-1, 128, 40, 60]               0\n",
      "           Conv2d-37          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 40, 60]             256\n",
      "             ReLU-39          [-1, 128, 40, 60]               0\n",
      "           Conv2d-40          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 40, 60]             256\n",
      "             ReLU-42          [-1, 128, 40, 60]               0\n",
      "         ResBlock-43          [-1, 128, 40, 60]               0\n",
      "           Conv2d-44          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 40, 60]             256\n",
      "             ReLU-46          [-1, 128, 40, 60]               0\n",
      "           Conv2d-47          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-48          [-1, 128, 40, 60]             256\n",
      "             ReLU-49          [-1, 128, 40, 60]               0\n",
      "         ResBlock-50          [-1, 128, 40, 60]               0\n",
      "           Conv2d-51          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 40, 60]             256\n",
      "             ReLU-53          [-1, 128, 40, 60]               0\n",
      "           Conv2d-54          [-1, 128, 40, 60]         147,456\n",
      "      BatchNorm2d-55          [-1, 128, 40, 60]             256\n",
      "             ReLU-56          [-1, 128, 40, 60]               0\n",
      "         ResBlock-57          [-1, 128, 40, 60]               0\n",
      "           Conv2d-58          [-1, 128, 40, 60]          16,512\n",
      "           Conv2d-59          [-1, 256, 20, 30]         294,912\n",
      "      BatchNorm2d-60          [-1, 256, 20, 30]             512\n",
      "             ReLU-61          [-1, 256, 20, 30]               0\n",
      "           Conv2d-62          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-63          [-1, 256, 20, 30]             512\n",
      "           Conv2d-64          [-1, 256, 20, 30]          32,768\n",
      "      BatchNorm2d-65          [-1, 256, 20, 30]             512\n",
      "             ReLU-66          [-1, 256, 20, 30]               0\n",
      "         ResBlock-67          [-1, 256, 20, 30]               0\n",
      "           Conv2d-68          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 20, 30]             512\n",
      "             ReLU-70          [-1, 256, 20, 30]               0\n",
      "           Conv2d-71          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-72          [-1, 256, 20, 30]             512\n",
      "             ReLU-73          [-1, 256, 20, 30]               0\n",
      "         ResBlock-74          [-1, 256, 20, 30]               0\n",
      "           Conv2d-75          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 20, 30]             512\n",
      "             ReLU-77          [-1, 256, 20, 30]               0\n",
      "           Conv2d-78          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-79          [-1, 256, 20, 30]             512\n",
      "             ReLU-80          [-1, 256, 20, 30]               0\n",
      "         ResBlock-81          [-1, 256, 20, 30]               0\n",
      "           Conv2d-82          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 20, 30]             512\n",
      "             ReLU-84          [-1, 256, 20, 30]               0\n",
      "           Conv2d-85          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-86          [-1, 256, 20, 30]             512\n",
      "             ReLU-87          [-1, 256, 20, 30]               0\n",
      "         ResBlock-88          [-1, 256, 20, 30]               0\n",
      "           Conv2d-89          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 20, 30]             512\n",
      "             ReLU-91          [-1, 256, 20, 30]               0\n",
      "           Conv2d-92          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 20, 30]             512\n",
      "             ReLU-94          [-1, 256, 20, 30]               0\n",
      "         ResBlock-95          [-1, 256, 20, 30]               0\n",
      "           Conv2d-96          [-1, 256, 20, 30]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 20, 30]             512\n",
      "             ReLU-98          [-1, 256, 20, 30]               0\n",
      "           Conv2d-99          [-1, 256, 20, 30]         589,824\n",
      "     BatchNorm2d-100          [-1, 256, 20, 30]             512\n",
      "            ReLU-101          [-1, 256, 20, 30]               0\n",
      "        ResBlock-102          [-1, 256, 20, 30]               0\n",
      "          Conv2d-103          [-1, 128, 20, 30]          32,896\n",
      "          Conv2d-104          [-1, 512, 10, 15]       1,179,648\n",
      "     BatchNorm2d-105          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-106          [-1, 512, 10, 15]               0\n",
      "          Conv2d-107          [-1, 512, 10, 15]       2,359,296\n",
      "     BatchNorm2d-108          [-1, 512, 10, 15]           1,024\n",
      "          Conv2d-109          [-1, 512, 10, 15]         131,072\n",
      "     BatchNorm2d-110          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-111          [-1, 512, 10, 15]               0\n",
      "        ResBlock-112          [-1, 512, 10, 15]               0\n",
      "          Conv2d-113          [-1, 512, 10, 15]       2,359,296\n",
      "     BatchNorm2d-114          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-115          [-1, 512, 10, 15]               0\n",
      "          Conv2d-116          [-1, 512, 10, 15]       2,359,296\n",
      "     BatchNorm2d-117          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-118          [-1, 512, 10, 15]               0\n",
      "        ResBlock-119          [-1, 512, 10, 15]               0\n",
      "          Conv2d-120          [-1, 512, 10, 15]       2,359,296\n",
      "     BatchNorm2d-121          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-122          [-1, 512, 10, 15]               0\n",
      "          Conv2d-123          [-1, 512, 10, 15]       2,359,296\n",
      "     BatchNorm2d-124          [-1, 512, 10, 15]           1,024\n",
      "            ReLU-125          [-1, 512, 10, 15]               0\n",
      "        ResBlock-126          [-1, 512, 10, 15]               0\n",
      "          Conv2d-127          [-1, 512, 10, 15]         262,656\n",
      " ConvTranspose2d-128          [-1, 128, 20, 30]         262,272\n",
      "            ReLU-129          [-1, 256, 20, 30]               0\n",
      "     BatchNorm2d-130          [-1, 256, 20, 30]             512\n",
      "AdaptiveAvgPool2d-131            [-1, 256, 1, 1]               0\n",
      "          Linear-132                  [-1, 128]          32,896\n",
      "            ReLU-133                  [-1, 128]               0\n",
      "          Linear-134                  [-1, 256]          33,024\n",
      "         Sigmoid-135                  [-1, 256]               0\n",
      "        CSEBlock-136          [-1, 256, 20, 30]               0\n",
      "          Conv2d-137            [-1, 1, 20, 30]             257\n",
      "         Sigmoid-138            [-1, 1, 20, 30]               0\n",
      "        SSEBlock-139          [-1, 256, 20, 30]               0\n",
      "       SCSEBlock-140          [-1, 256, 20, 30]               0\n",
      "     UpsampBlock-141          [-1, 256, 20, 30]               0\n",
      " ConvTranspose2d-142          [-1, 128, 40, 60]         131,200\n",
      "            ReLU-143          [-1, 256, 40, 60]               0\n",
      "     BatchNorm2d-144          [-1, 256, 40, 60]             512\n",
      "AdaptiveAvgPool2d-145            [-1, 256, 1, 1]               0\n",
      "          Linear-146                  [-1, 128]          32,896\n",
      "            ReLU-147                  [-1, 128]               0\n",
      "          Linear-148                  [-1, 256]          33,024\n",
      "         Sigmoid-149                  [-1, 256]               0\n",
      "        CSEBlock-150          [-1, 256, 40, 60]               0\n",
      "          Conv2d-151            [-1, 1, 40, 60]             257\n",
      "         Sigmoid-152            [-1, 1, 40, 60]               0\n",
      "        SSEBlock-153          [-1, 256, 40, 60]               0\n",
      "       SCSEBlock-154          [-1, 256, 40, 60]               0\n",
      "     UpsampBlock-155          [-1, 256, 40, 60]               0\n",
      " ConvTranspose2d-156         [-1, 128, 80, 120]         131,200\n",
      "            ReLU-157         [-1, 256, 80, 120]               0\n",
      "     BatchNorm2d-158         [-1, 256, 80, 120]             512\n",
      "AdaptiveAvgPool2d-159            [-1, 256, 1, 1]               0\n",
      "          Linear-160                  [-1, 128]          32,896\n",
      "            ReLU-161                  [-1, 128]               0\n",
      "          Linear-162                  [-1, 256]          33,024\n",
      "         Sigmoid-163                  [-1, 256]               0\n",
      "        CSEBlock-164         [-1, 256, 80, 120]               0\n",
      "          Conv2d-165           [-1, 1, 80, 120]             257\n",
      "         Sigmoid-166           [-1, 1, 80, 120]               0\n",
      "        SSEBlock-167         [-1, 256, 80, 120]               0\n",
      "       SCSEBlock-168         [-1, 256, 80, 120]               0\n",
      "     UpsampBlock-169         [-1, 256, 80, 120]               0\n",
      " ConvTranspose2d-170        [-1, 128, 160, 240]         131,200\n",
      "            ReLU-171        [-1, 256, 160, 240]               0\n",
      "     BatchNorm2d-172        [-1, 256, 160, 240]             512\n",
      "AdaptiveAvgPool2d-173            [-1, 256, 1, 1]               0\n",
      "          Linear-174                  [-1, 128]          32,896\n",
      "            ReLU-175                  [-1, 128]               0\n",
      "          Linear-176                  [-1, 256]          33,024\n",
      "         Sigmoid-177                  [-1, 256]               0\n",
      "        CSEBlock-178        [-1, 256, 160, 240]               0\n",
      "          Conv2d-179          [-1, 1, 160, 240]             257\n",
      "         Sigmoid-180          [-1, 1, 160, 240]               0\n",
      "        SSEBlock-181        [-1, 256, 160, 240]               0\n",
      "       SCSEBlock-182        [-1, 256, 160, 240]               0\n",
      "     UpsampBlock-183        [-1, 256, 160, 240]               0\n",
      " ConvTranspose2d-184          [-1, 1, 320, 480]           1,025\n",
      "================================================================\n",
      "Total params: 22,537,029\n",
      "Trainable params: 22,537,029\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.76\n",
      "Forward/backward pass size (MB): 994.56\n",
      "Params size (MB): 85.97\n",
      "Estimated Total Size (MB): 1082.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from architecture import main\n",
    "from torchsummary import summary\n",
    "\n",
    "network = main.Net()\n",
    "# print(network)\n",
    "summary(network, input_size=(3, 320, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00002-0c6e2e3b-db59-4681-ab21-9c6526eb0f05",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1616745886878,
    "source_hash": "aba374a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is size:  118 \n",
      "The current split is train/test:  71 / 47\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets.CFD.CFDdata import CFD\n",
    "\n",
    "current_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "totensor = transforms.ToTensor()\n",
    "dataset = CFD(current_path + \"/datasets/CFD/cfd_image/\", totensor, current_path + \"/datasets/CFD/seg_gt/\", totensor)\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_data, test_data = random_split(dataset, [71, 47], gen)\n",
    "train_loader = DataLoader(train_data, batch_size=10)\n",
    "test_loader = DataLoader(test_data, batch_size=47)\n",
    "\n",
    "print(\"The current dataset is size: \", len(dataset), \n",
    "\"\\nThe current split is train/test: \", len(train_loader.dataset), \"/\", len(test_loader.dataset))\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
    "# for i in range(25):\n",
    "#     _, gt = test_data[i]\n",
    "#     ax = axs[i // 5][i % 5]\n",
    "#     ax.imshow(gt.view(320, 480), cmap=\"gray\")\n",
    "#     ax.axis(\"off\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00004-7428292d-49f2-411d-8364-8347593a3c9e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 68,
    "execution_start": 1616684409029,
    "source_hash": "d3235d1a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some functions used during training\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"\n",
    "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
    "    as cpu.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_loader, network, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    network.eval()  #make sure network is in evaluation mode\n",
    "\n",
    "    #init\n",
    "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
    "    n = 0\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        # Copy the data to device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(network(X), dim=1) == y))\n",
    "            n += y.shape[0] #increases with the number of samples in the batch\n",
    "    return acc_sum.item()/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00004-a65cc30d-dddc-41df-9d94-28845042d191",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 45,
    "execution_start": 1616684471202,
    "output_cleared": true,
    "source_hash": null,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Accuracy of train set: 148975763%\n",
      "Accuracy of test set: 711764289%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loss import dice_loss\n",
    "\n",
    "# training parameters\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "#Initialize network\n",
    "network = main.Net()\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "criterion = dice_loss.batch_dice_loss\n",
    "\n",
    "# Define list to store losses and performances of each interation\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Try using gpu instead of cpu\n",
    "device = try_gpu()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Network in training mode and to device\n",
    "    network.train()\n",
    "    network.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "\n",
    "         # Set to same device\n",
    "         x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "         # Set the gradients to zero\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "         # Perform forward pass\n",
    "         y_pred = network(x_batch)\n",
    "\n",
    "         # Compute the loss\n",
    "         loss = criterion(y_pred, y_batch)\n",
    "         train_losses.append(loss)\n",
    "\n",
    "         # Backward computation and update\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "    \n",
    "    # Compute train and test error\n",
    "    train_acc = 100*evaluate_accuracy(train_loader, network.to(device), device=device)\n",
    "    test_acc = 100*evaluate_accuracy(test_loader, network.to(device), device=device)\n",
    "    \n",
    "    # Development of performance\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    # Print performance\n",
    "    print('Epoch: {:.0f}'.format(epoch+1))\n",
    "    print('Accuracy of train set: {:.00f}%'.format(train_acc))\n",
    "    print('Accuracy of test set: {:.00f}%'.format(test_acc))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-ee16a739-e3a0-4689-a9ed-33c171733c59",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1981,
    "output_cleared": true,
    "source_hash": null,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# x = torch.randn(1,3, 320, 480)\n",
    "# y = net(x) \n",
    "\n",
    "# make_dot(y).view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=411d58e9-cb4b-4924-bef0-2f383eff0187' target=\"_blank\">\n",
    "<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "96775278-d606-4bc7-a96c-64fcdaf37c69",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
