{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reproduce DL\n## Automated Pavement Crack Segmentation",
      "metadata": {
        "tags": [],
        "cell_id": "00000-82d331e0-79e4-40f0-ba43-0ad2ccc7fea5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-0be6212f-ba68-4778-be63-87a42300ab47",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ca8ef026",
        "execution_millis": 1600,
        "execution_start": 1616402949806,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Residual block (Green)\n    ---\n    A special case of highway network without any gates\n    in their skip connection. Thus allowing the flow of\n    memory (or info) from initial layers to last layers.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResBlock, self).__init__()\n        self.stride_one = (stride == 1)\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.conv_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        self.bn_shortcut = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu(y)\n        y = self.conv2(y)\n        y = self.bn2(y)\n\n        if not self.stride_one:\n            x = self.bn_shortcut(self.conv_shortcut(x))\n        y += x\n\n        return self.relu(y)\n\n\nclass CSEBlock(nn.Module):\n    \"\"\"\n    Spatial Squeeze and Channel Excitation block\n    ---\n    Channel-wise focus\n\n    Recalibrates the channels by incorporating global\n    spatial information. It provides a receptive field\n    of whole spatial extent at the fc's.\n\n    Assign each channel (feature) of a convolutional \n    block (feature map) a different weightage \n    (excitation) based on how important each channel \n    is (squeeze) instead of equally weighing each\n    feature. This improves channel interdependencies.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(CSEBlock, self).__init__()\n        # Global pooling == AdaptiveAvgPool2d\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, _, _ = x.size()\n\n        avg_pool_x = self.avg_pool(x).view(batch_size, num_channels)\n\n        y = self.fc1(avg_pool_x)\n        y = self.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, num_channels, 1, 1)\n\n\nclass SSEBlock(nn.Module):\n    \"\"\"\n    Channel Squeeze and Spatial Excitation block\n    ---\n    Spatial-wise focus\n\n    It behaves like a spatial attention map indicating\n    where the network should focus more to aid the\n    segmentation.\n\n    Assign importance to spatial locations sort of \n    telling where features are better to focus\n    instead of reweighing which features are more\n    important.\n    \"\"\"\n\n    def __init__(self, in_channels):\n        super(SSEBlock, self).__init__()\n        # Output channel = 1, 1x1 convolution\n        self.conv = nn.Conv2d(in_channels, 1, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, H, W = x.size()\n\n        y = self.conv(x)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, 1, H, W)\n\n\nclass SCSEBlock(nn.Module):\n    \"\"\"\n    Spatial and Channel Squeeze and Excitation block\n    ---\n    Return the block with the most promising values.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(SCSEBlock, self).__init__()\n        self.CSE = CSEBlock(in_channels, reduction)\n        self.SSE = SSEBlock(in_channels)\n\n    def forward(self, x):\n        return torch.max(self.CSE(x), self.SSE(x))\n\n\nclass UpsampBlock(nn.Module):\n    \"\"\"\n    Upsampling block\n    ---\n    Includes SCSEBlock\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super(UpsampBlock, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scse = SCSEBlock(in_channels)\n\n    def forward(self, x):\n        y = self.relu(x)\n        y = self.bn(y)\n\n        return self.scse(y)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # 3 input image channels, 64 output channels, 7x7 convolution, stride 2\n        # -- Blue --\n        self.conv1 = nn.Conv2d(3, 64, 7, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        # -- Green --\n        # maxpooling stride default is kernel size, ceil_mode added for padding\n        self.maxpool_1_to_rs1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        # Residual block 1\n        self.conv_rs1_1 = ResBlock(64, 64, 1)\n        self.conv_rs1_2 = ResBlock(64, 64, 1)\n        self.conv_rs1_3 = ResBlock(64, 64, 1)\n\n        # Residual block 2\n        self.conv_rs2_1 = ResBlock(64, 128, 2)\n        self.conv_rs2_2 = ResBlock(128, 128, 1)\n        self.conv_rs2_3 = ResBlock(128, 128, 1)\n        self.conv_rs2_4 = ResBlock(128, 128, 1)\n\n        # Residual block 3\n        self.conv_rs3_1 = ResBlock(128, 256, 2)\n        self.conv_rs3_2 = ResBlock(256, 256, 1)\n        self.conv_rs3_3 = ResBlock(256, 256, 1)\n        self.conv_rs3_4 = ResBlock(256, 256, 1)\n        self.conv_rs3_5 = ResBlock(256, 256, 1)\n        self.conv_rs3_6 = ResBlock(256, 256, 1)\n\n        # Residual block 4\n        self.conv_rs4_1 = ResBlock(256, 512, 2)\n        self.conv_rs4_2 = ResBlock(512, 512, 1)\n        self.conv_rs4_3 = ResBlock(512, 512, 1)\n\n        # -- Yellow --\n        # 64/128/256 input image channels, 128 output channels, 1x1 convolution, stride 1\n        # Green (residual) block to Yellow block (Up to down)\n        self.conv_gr_to_yel_1 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_2 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_3 = nn.Conv2d(128, 128, 1, stride=1)\n        self.conv_gr_to_yel_4 = nn.Conv2d(256, 128, 1, stride=1)\n\n        # -- Purple --\n        # 512 input image channels, 512 output channels, 1x1 convolution, stride 1\n        # Green (residual) block to Purple block\n        self.conv_gr_to_purp = nn.Conv2d(512, 512, 1, stride=1)\n        # Magenta block to Purple block\n        self.conv_mag_to_purp_1 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_2 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_3 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_4 = UpsampBlock(256, 256)\n\n        # -- Magenta --\n        # 512/256 input image channels, 128 output channels, 2x2 convolution, stride 2\n        # Purple (residual) block to Magenta block (Down to up)\n        self.conv_purp_to_mag_1 = nn.ConvTranspose2d(512, 128, 2, stride=2)\n        self.conv_purp_to_mag_2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_4 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n\n    def forward(self, x):\n        # Add \"Same\"-padding like Keras.\n        x = F.pad(x, (2, 3, 2, 3))\n\n        # Traverse down the architecture (ResNet34)\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu1(y)\n\n        y_y1 = self.conv_gr_to_yel_1(y)\n\n        y = self.maxpool_1_to_rs1(y)\n        y = self.conv_rs1_1(y)\n        y = self.conv_rs1_2(y)\n        y = self.conv_rs1_3(y)\n        \n        y_y2 = self.conv_gr_to_yel_2(y)\n\n        y = self.conv_rs2_1(y)\n        y = self.conv_rs2_2(y)\n        y = self.conv_rs2_3(y)\n        y = self.conv_rs2_4(y)\n\n        y_y3 = self.conv_gr_to_yel_3(y)\n\n        y = self.conv_rs3_1(y)\n        y = self.conv_rs3_2(y)\n        y = self.conv_rs3_3(y)\n        y = self.conv_rs3_4(y)\n        y = self.conv_rs3_5(y)\n        y = self.conv_rs3_6(y)\n\n        y_y4 = self.conv_gr_to_yel_4(y)\n\n        y = self.conv_rs4_1(y)\n        y = self.conv_rs4_2(y)\n        y = self.conv_rs4_3(y)\n\n        y_y5 = self.conv_gr_to_purp(y)\n\n        # Traverse up the U-based architecture\n        y_y5 = self.conv_purp_to_mag_1(y_y5)\n\n        y_y4 = torch.cat((y_y4, y_y5), dim=1)\n        y_y4 = self.conv_mag_to_purp_1(y_y4)\n        y_y4 = self.conv_purp_to_mag_2(y_y4)\n\n        y_y3 = torch.cat((y_y3, y_y4), dim=1)\n        y_y3 = self.conv_mag_to_purp_2(y_y3)\n        y_y3 = self.conv_purp_to_mag_3(y_y3)\n\n        y_y2 = torch.cat((y_y2, y_y3), dim=1)\n        y_y2 = self.conv_mag_to_purp_3(y_y2)\n        y_y2 = self.conv_purp_to_mag_4(y_y2)\n\n        y_y1 = torch.cat((y_y1, y_y2), dim=1)\n        y_y1 = self.conv_mag_to_purp_4(y_y1)\n        return self.conv_purp_to_mag_5(y_y1)\n\n\nnet = Net()\n# print(net)\nsummary(net, input_size=(3, 320, 480))",
      "execution_count": 40,
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 160, 240]           9,472\n       BatchNorm2d-2         [-1, 64, 160, 240]             128\n              ReLU-3         [-1, 64, 160, 240]               0\n            Conv2d-4        [-1, 128, 160, 240]           8,320\n         MaxPool2d-5          [-1, 64, 80, 120]               0\n            Conv2d-6          [-1, 64, 80, 120]          36,928\n       BatchNorm2d-7          [-1, 64, 80, 120]             128\n              ReLU-8          [-1, 64, 80, 120]               0\n            Conv2d-9          [-1, 64, 80, 120]          36,928\n      BatchNorm2d-10          [-1, 64, 80, 120]             128\n             ReLU-11          [-1, 64, 80, 120]               0\n         ResBlock-12          [-1, 64, 80, 120]               0\n           Conv2d-13          [-1, 64, 80, 120]          36,928\n      BatchNorm2d-14          [-1, 64, 80, 120]             128\n             ReLU-15          [-1, 64, 80, 120]               0\n           Conv2d-16          [-1, 64, 80, 120]          36,928\n      BatchNorm2d-17          [-1, 64, 80, 120]             128\n             ReLU-18          [-1, 64, 80, 120]               0\n         ResBlock-19          [-1, 64, 80, 120]               0\n           Conv2d-20          [-1, 64, 80, 120]          36,928\n      BatchNorm2d-21          [-1, 64, 80, 120]             128\n             ReLU-22          [-1, 64, 80, 120]               0\n           Conv2d-23          [-1, 64, 80, 120]          36,928\n      BatchNorm2d-24          [-1, 64, 80, 120]             128\n             ReLU-25          [-1, 64, 80, 120]               0\n         ResBlock-26          [-1, 64, 80, 120]               0\n           Conv2d-27         [-1, 128, 80, 120]           8,320\n           Conv2d-28          [-1, 128, 40, 60]          73,856\n      BatchNorm2d-29          [-1, 128, 40, 60]             256\n             ReLU-30          [-1, 128, 40, 60]               0\n           Conv2d-31          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-32          [-1, 128, 40, 60]             256\n           Conv2d-33          [-1, 128, 40, 60]           8,320\n      BatchNorm2d-34          [-1, 128, 40, 60]             256\n             ReLU-35          [-1, 128, 40, 60]               0\n         ResBlock-36          [-1, 128, 40, 60]               0\n           Conv2d-37          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-38          [-1, 128, 40, 60]             256\n             ReLU-39          [-1, 128, 40, 60]               0\n           Conv2d-40          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-41          [-1, 128, 40, 60]             256\n             ReLU-42          [-1, 128, 40, 60]               0\n         ResBlock-43          [-1, 128, 40, 60]               0\n           Conv2d-44          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-45          [-1, 128, 40, 60]             256\n             ReLU-46          [-1, 128, 40, 60]               0\n           Conv2d-47          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-48          [-1, 128, 40, 60]             256\n             ReLU-49          [-1, 128, 40, 60]               0\n         ResBlock-50          [-1, 128, 40, 60]               0\n           Conv2d-51          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-52          [-1, 128, 40, 60]             256\n             ReLU-53          [-1, 128, 40, 60]               0\n           Conv2d-54          [-1, 128, 40, 60]         147,584\n      BatchNorm2d-55          [-1, 128, 40, 60]             256\n             ReLU-56          [-1, 128, 40, 60]               0\n         ResBlock-57          [-1, 128, 40, 60]               0\n           Conv2d-58          [-1, 128, 40, 60]          16,512\n           Conv2d-59          [-1, 256, 20, 30]         295,168\n      BatchNorm2d-60          [-1, 256, 20, 30]             512\n             ReLU-61          [-1, 256, 20, 30]               0\n           Conv2d-62          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-63          [-1, 256, 20, 30]             512\n           Conv2d-64          [-1, 256, 20, 30]          33,024\n      BatchNorm2d-65          [-1, 256, 20, 30]             512\n             ReLU-66          [-1, 256, 20, 30]               0\n         ResBlock-67          [-1, 256, 20, 30]               0\n           Conv2d-68          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-69          [-1, 256, 20, 30]             512\n             ReLU-70          [-1, 256, 20, 30]               0\n           Conv2d-71          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-72          [-1, 256, 20, 30]             512\n             ReLU-73          [-1, 256, 20, 30]               0\n         ResBlock-74          [-1, 256, 20, 30]               0\n           Conv2d-75          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-76          [-1, 256, 20, 30]             512\n             ReLU-77          [-1, 256, 20, 30]               0\n           Conv2d-78          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-79          [-1, 256, 20, 30]             512\n             ReLU-80          [-1, 256, 20, 30]               0\n         ResBlock-81          [-1, 256, 20, 30]               0\n           Conv2d-82          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-83          [-1, 256, 20, 30]             512\n             ReLU-84          [-1, 256, 20, 30]               0\n           Conv2d-85          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-86          [-1, 256, 20, 30]             512\n             ReLU-87          [-1, 256, 20, 30]               0\n         ResBlock-88          [-1, 256, 20, 30]               0\n           Conv2d-89          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-90          [-1, 256, 20, 30]             512\n             ReLU-91          [-1, 256, 20, 30]               0\n           Conv2d-92          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-93          [-1, 256, 20, 30]             512\n             ReLU-94          [-1, 256, 20, 30]               0\n         ResBlock-95          [-1, 256, 20, 30]               0\n           Conv2d-96          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-97          [-1, 256, 20, 30]             512\n             ReLU-98          [-1, 256, 20, 30]               0\n           Conv2d-99          [-1, 256, 20, 30]         590,080\n     BatchNorm2d-100          [-1, 256, 20, 30]             512\n            ReLU-101          [-1, 256, 20, 30]               0\n        ResBlock-102          [-1, 256, 20, 30]               0\n          Conv2d-103          [-1, 128, 20, 30]          32,896\n          Conv2d-104          [-1, 512, 10, 15]       1,180,160\n     BatchNorm2d-105          [-1, 512, 10, 15]           1,024\n            ReLU-106          [-1, 512, 10, 15]               0\n          Conv2d-107          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-108          [-1, 512, 10, 15]           1,024\n          Conv2d-109          [-1, 512, 10, 15]         131,584\n     BatchNorm2d-110          [-1, 512, 10, 15]           1,024\n            ReLU-111          [-1, 512, 10, 15]               0\n        ResBlock-112          [-1, 512, 10, 15]               0\n          Conv2d-113          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-114          [-1, 512, 10, 15]           1,024\n            ReLU-115          [-1, 512, 10, 15]               0\n          Conv2d-116          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-117          [-1, 512, 10, 15]           1,024\n            ReLU-118          [-1, 512, 10, 15]               0\n        ResBlock-119          [-1, 512, 10, 15]               0\n          Conv2d-120          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-121          [-1, 512, 10, 15]           1,024\n            ReLU-122          [-1, 512, 10, 15]               0\n          Conv2d-123          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-124          [-1, 512, 10, 15]           1,024\n            ReLU-125          [-1, 512, 10, 15]               0\n        ResBlock-126          [-1, 512, 10, 15]               0\n          Conv2d-127          [-1, 512, 10, 15]         262,656\n ConvTranspose2d-128          [-1, 128, 20, 30]         262,272\n            ReLU-129          [-1, 256, 20, 30]               0\n     BatchNorm2d-130          [-1, 256, 20, 30]             512\nAdaptiveAvgPool2d-131            [-1, 256, 1, 1]               0\n          Linear-132                  [-1, 128]          32,896\n            ReLU-133                  [-1, 128]               0\n          Linear-134                  [-1, 256]          33,024\n         Sigmoid-135                  [-1, 256]               0\n        CSEBlock-136          [-1, 256, 20, 30]               0\n          Conv2d-137            [-1, 1, 20, 30]             257\n         Sigmoid-138            [-1, 1, 20, 30]               0\n        SSEBlock-139          [-1, 256, 20, 30]               0\n       SCSEBlock-140          [-1, 256, 20, 30]               0\n     UpsampBlock-141          [-1, 256, 20, 30]               0\n ConvTranspose2d-142          [-1, 128, 40, 60]         131,200\n            ReLU-143          [-1, 256, 40, 60]               0\n     BatchNorm2d-144          [-1, 256, 40, 60]             512\nAdaptiveAvgPool2d-145            [-1, 256, 1, 1]               0\n          Linear-146                  [-1, 128]          32,896\n            ReLU-147                  [-1, 128]               0\n          Linear-148                  [-1, 256]          33,024\n         Sigmoid-149                  [-1, 256]               0\n        CSEBlock-150          [-1, 256, 40, 60]               0\n          Conv2d-151            [-1, 1, 40, 60]             257\n         Sigmoid-152            [-1, 1, 40, 60]               0\n        SSEBlock-153          [-1, 256, 40, 60]               0\n       SCSEBlock-154          [-1, 256, 40, 60]               0\n     UpsampBlock-155          [-1, 256, 40, 60]               0\n ConvTranspose2d-156         [-1, 128, 80, 120]         131,200\n            ReLU-157         [-1, 256, 80, 120]               0\n     BatchNorm2d-158         [-1, 256, 80, 120]             512\nAdaptiveAvgPool2d-159            [-1, 256, 1, 1]               0\n          Linear-160                  [-1, 128]          32,896\n            ReLU-161                  [-1, 128]               0\n          Linear-162                  [-1, 256]          33,024\n         Sigmoid-163                  [-1, 256]               0\n        CSEBlock-164         [-1, 256, 80, 120]               0\n          Conv2d-165           [-1, 1, 80, 120]             257\n         Sigmoid-166           [-1, 1, 80, 120]               0\n        SSEBlock-167         [-1, 256, 80, 120]               0\n       SCSEBlock-168         [-1, 256, 80, 120]               0\n     UpsampBlock-169         [-1, 256, 80, 120]               0\n ConvTranspose2d-170        [-1, 128, 160, 240]         131,200\n            ReLU-171        [-1, 256, 160, 240]               0\n     BatchNorm2d-172        [-1, 256, 160, 240]             512\nAdaptiveAvgPool2d-173            [-1, 256, 1, 1]               0\n          Linear-174                  [-1, 128]          32,896\n            ReLU-175                  [-1, 128]               0\n          Linear-176                  [-1, 256]          33,024\n         Sigmoid-177                  [-1, 256]               0\n        CSEBlock-178        [-1, 256, 160, 240]               0\n          Conv2d-179          [-1, 1, 160, 240]             257\n         Sigmoid-180          [-1, 1, 160, 240]               0\n        SSEBlock-181        [-1, 256, 160, 240]               0\n       SCSEBlock-182        [-1, 256, 160, 240]               0\n     UpsampBlock-183        [-1, 256, 160, 240]               0\n ConvTranspose2d-184          [-1, 1, 320, 480]           1,025\n================================================================\nTotal params: 22,545,541\nTrainable params: 22,545,541\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.76\nForward/backward pass size (MB): 994.56\nParams size (MB): 86.00\nEstimated Total Size (MB): 1082.32\n----------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torchsummary import summary\n\n# Use pretrained model, extend with U-architecture &  SCSE\npre_orig_model = models.resnet34(pretrained=True)\npre_model = torch.nn.Sequential(*(list(pre_orig_model.children())[:-2]))\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Residual block (Green)\n    ---\n    A special case of highway network without any gates\n    in their skip connection. Thus allowing the flow of\n    memory (or info) from initial layers to last layers.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResBlock, self).__init__()\n        self.stride_one = (stride == 1)\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.conv_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        self.bn_shortcut = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu(y)\n        y = self.conv2(y)\n        y = self.bn2(y)\n\n        if not self.stride_one:\n            x = self.bn_shortcut(self.conv_shortcut(x))\n        y += x\n\n        return self.relu(y)\n\n\nclass CSEBlock(nn.Module):\n    \"\"\"\n    Spatial Squeeze and Channel Excitation block\n    ---\n    Channel-wise focus\n\n    Recalibrates the channels by incorporating global\n    spatial information. It provides a receptive field\n    of whole spatial extent at the fc's.\n\n    Assign each channel (feature) of a convolutional\n    block (feature map) a different weightage\n    (excitation) based on how important each channel\n    is (squeeze) instead of equally weighing each\n    feature. This improves channel interdependencies.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(CSEBlock, self).__init__()\n        # Global pooling == AdaptiveAvgPool2d\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, _, _ = x.size()\n\n        avg_pool_x = self.avg_pool(x).view(batch_size, num_channels)\n\n        y = self.fc1(avg_pool_x)\n        y = self.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, num_channels, 1, 1)\n\n\nclass SSEBlock(nn.Module):\n    \"\"\"\n    Channel Squeeze and Spatial Excitation block\n    ---\n    Spatial-wise focus\n\n    It behaves like a spatial attention map indicating\n    where the network should focus more to aid the\n    segmentation.\n\n    Assign importance to spatial locations sort of\n    telling where features are better to focus\n    instead of reweighing which features are more\n    important.\n    \"\"\"\n\n    def __init__(self, in_channels):\n        super(SSEBlock, self).__init__()\n        # Output channel = 1, 1x1 convolution\n        self.conv = nn.Conv2d(in_channels, 1, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, H, W = x.size()\n\n        y = self.conv(x)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, 1, H, W)\n\n\nclass SCSEBlock(nn.Module):\n    \"\"\"\n    Spatial and Channel Squeeze and Excitation block\n    ---\n    Return the block with the most promising values.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(SCSEBlock, self).__init__()\n        self.CSE = CSEBlock(in_channels, reduction)\n        self.SSE = SSEBlock(in_channels)\n\n    def forward(self, x):\n        return torch.max(self.CSE(x), self.SSE(x))\n\n\nclass UpsampBlock(nn.Module):\n    \"\"\"\n    Upsampling block\n    ---\n    Includes SCSEBlock\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super(UpsampBlock, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scse = SCSEBlock(in_channels)\n\n    def forward(self, x):\n        y = self.relu(x)\n        y = self.bn(y)\n\n        return self.scse(y)\n\n\nclass NetBlock(nn.Module):\n\n    def __init__(self):\n        super(NetBlock, self).__init__()\n\n        # 3 input image channels, 64 output channels, 7x7 convolution, stride 2\n        # -- Blue --\n        self.blueBlock = torch.nn.Sequential(*(list(pre_model.children())[:3]))\n\n        # -- Green --\n        # maxpooling stride default is kernel size\n        self.resBlock1 = torch.nn.Sequential(*(list(pre_model.children())[3:5]))\n        self.resBlock2 = torch.nn.Sequential(*(list(pre_model.children())[5:6]))\n        self.resBlock3 = torch.nn.Sequential(*(list(pre_model.children())[6:7]))\n        self.resBlock4 = torch.nn.Sequential(*(list(pre_model.children())[7:]))\n\n        # -- Yellow --\n        # 64/128/256 input image channels, 128 output channels, 1x1 convolution, stride 1\n        # Green (residual) block to Yellow block (Up to down)\n        self.conv_gr_to_yel_1 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_2 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_3 = nn.Conv2d(128, 128, 1, stride=1)\n        self.conv_gr_to_yel_4 = nn.Conv2d(256, 128, 1, stride=1)\n\n        # -- Purple --\n        # 512 input image channels, 512 output channels, 1x1 convolution, stride 1\n        # Green (residual) block to Purple block\n        self.conv_gr_to_purp = nn.Conv2d(512, 512, 1, stride=1)\n        # Magenta block to Purple block\n        self.conv_mag_to_purp_1 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_2 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_3 = UpsampBlock(256, 256)\n        self.conv_mag_to_purp_4 = UpsampBlock(256, 256)\n\n        # -- Magenta --\n        # 512/256 input image channels, 128 output channels, 2x2 convolution, stride 2\n        # Purple (residual) block to Magenta block (Down to up)\n        self.conv_purp_to_mag_1 = nn.ConvTranspose2d(512, 128, 2, stride=2)\n        self.conv_purp_to_mag_2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_4 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv_purp_to_mag_5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n\n    def forward(self, x):\n        # Traverse down the architecture (ResNet34)\n        y = self.blueBlock(x)\n        y_y1 = self.conv_gr_to_yel_1(y)\n\n        y = self.resBlock1(y)\n        y_y2 = self.conv_gr_to_yel_2(y)\n\n        y = self.resBlock2(y)\n        y_y3 = self.conv_gr_to_yel_3(y)\n\n        y = self.resBlock3(y)\n        y_y4 = self.conv_gr_to_yel_4(y)\n\n        y = self.resBlock4(y)\n        y_y5 = self.conv_gr_to_purp(y)\n\n        # Traverse up the U-based architecture\n        y_y5 = self.conv_purp_to_mag_1(y_y5)\n\n        y_y4 = torch.cat((y_y4, y_y5), dim=1)\n        y_y4 = self.conv_mag_to_purp_1(y_y4)\n        y_y4 = self.conv_purp_to_mag_2(y_y4)\n\n        y_y3 = torch.cat((y_y3, y_y4), dim=1)\n        y_y3 = self.conv_mag_to_purp_2(y_y3)\n        y_y3 = self.conv_purp_to_mag_3(y_y3)\n\n        y_y2 = torch.cat((y_y2, y_y3), dim=1)\n        y_y2 = self.conv_mag_to_purp_3(y_y2)\n        y_y2 = self.conv_purp_to_mag_4(y_y2)\n\n        y_y1 = torch.cat((y_y1, y_y2), dim=1)\n        y_y1 = self.conv_mag_to_purp_4(y_y1)\n        return self.conv_purp_to_mag_5(y_y1)\n\n\nnet = NetBlock()\n# print(net)\nsummary(net, input_size=(3, 320, 480))",
      "metadata": {
        "tags": [],
        "cell_id": "00002-da764467-46fa-4b0b-8c70-b51631d8659f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8f58990f",
        "execution_millis": 2034,
        "execution_start": 1616403482633,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 160, 240]           9,408\n       BatchNorm2d-2         [-1, 64, 160, 240]             128\n              ReLU-3         [-1, 64, 160, 240]               0\n            Conv2d-4        [-1, 128, 160, 240]           8,320\n         MaxPool2d-5          [-1, 64, 80, 120]               0\n            Conv2d-6          [-1, 64, 80, 120]          36,864\n       BatchNorm2d-7          [-1, 64, 80, 120]             128\n              ReLU-8          [-1, 64, 80, 120]               0\n            Conv2d-9          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-10          [-1, 64, 80, 120]             128\n             ReLU-11          [-1, 64, 80, 120]               0\n       BasicBlock-12          [-1, 64, 80, 120]               0\n           Conv2d-13          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-14          [-1, 64, 80, 120]             128\n             ReLU-15          [-1, 64, 80, 120]               0\n           Conv2d-16          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-17          [-1, 64, 80, 120]             128\n             ReLU-18          [-1, 64, 80, 120]               0\n       BasicBlock-19          [-1, 64, 80, 120]               0\n           Conv2d-20          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-21          [-1, 64, 80, 120]             128\n             ReLU-22          [-1, 64, 80, 120]               0\n           Conv2d-23          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-24          [-1, 64, 80, 120]             128\n             ReLU-25          [-1, 64, 80, 120]               0\n       BasicBlock-26          [-1, 64, 80, 120]               0\n           Conv2d-27         [-1, 128, 80, 120]           8,320\n           Conv2d-28          [-1, 128, 40, 60]          73,728\n      BatchNorm2d-29          [-1, 128, 40, 60]             256\n             ReLU-30          [-1, 128, 40, 60]               0\n           Conv2d-31          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-32          [-1, 128, 40, 60]             256\n           Conv2d-33          [-1, 128, 40, 60]           8,192\n      BatchNorm2d-34          [-1, 128, 40, 60]             256\n             ReLU-35          [-1, 128, 40, 60]               0\n       BasicBlock-36          [-1, 128, 40, 60]               0\n           Conv2d-37          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-38          [-1, 128, 40, 60]             256\n             ReLU-39          [-1, 128, 40, 60]               0\n           Conv2d-40          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-41          [-1, 128, 40, 60]             256\n             ReLU-42          [-1, 128, 40, 60]               0\n       BasicBlock-43          [-1, 128, 40, 60]               0\n           Conv2d-44          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-45          [-1, 128, 40, 60]             256\n             ReLU-46          [-1, 128, 40, 60]               0\n           Conv2d-47          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-48          [-1, 128, 40, 60]             256\n             ReLU-49          [-1, 128, 40, 60]               0\n       BasicBlock-50          [-1, 128, 40, 60]               0\n           Conv2d-51          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-52          [-1, 128, 40, 60]             256\n             ReLU-53          [-1, 128, 40, 60]               0\n           Conv2d-54          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-55          [-1, 128, 40, 60]             256\n             ReLU-56          [-1, 128, 40, 60]               0\n       BasicBlock-57          [-1, 128, 40, 60]               0\n           Conv2d-58          [-1, 128, 40, 60]          16,512\n           Conv2d-59          [-1, 256, 20, 30]         294,912\n      BatchNorm2d-60          [-1, 256, 20, 30]             512\n             ReLU-61          [-1, 256, 20, 30]               0\n           Conv2d-62          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-63          [-1, 256, 20, 30]             512\n           Conv2d-64          [-1, 256, 20, 30]          32,768\n      BatchNorm2d-65          [-1, 256, 20, 30]             512\n             ReLU-66          [-1, 256, 20, 30]               0\n       BasicBlock-67          [-1, 256, 20, 30]               0\n           Conv2d-68          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-69          [-1, 256, 20, 30]             512\n             ReLU-70          [-1, 256, 20, 30]               0\n           Conv2d-71          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-72          [-1, 256, 20, 30]             512\n             ReLU-73          [-1, 256, 20, 30]               0\n       BasicBlock-74          [-1, 256, 20, 30]               0\n           Conv2d-75          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-76          [-1, 256, 20, 30]             512\n             ReLU-77          [-1, 256, 20, 30]               0\n           Conv2d-78          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-79          [-1, 256, 20, 30]             512\n             ReLU-80          [-1, 256, 20, 30]               0\n       BasicBlock-81          [-1, 256, 20, 30]               0\n           Conv2d-82          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-83          [-1, 256, 20, 30]             512\n             ReLU-84          [-1, 256, 20, 30]               0\n           Conv2d-85          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-86          [-1, 256, 20, 30]             512\n             ReLU-87          [-1, 256, 20, 30]               0\n       BasicBlock-88          [-1, 256, 20, 30]               0\n           Conv2d-89          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-90          [-1, 256, 20, 30]             512\n             ReLU-91          [-1, 256, 20, 30]               0\n           Conv2d-92          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-93          [-1, 256, 20, 30]             512\n             ReLU-94          [-1, 256, 20, 30]               0\n       BasicBlock-95          [-1, 256, 20, 30]               0\n           Conv2d-96          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-97          [-1, 256, 20, 30]             512\n             ReLU-98          [-1, 256, 20, 30]               0\n           Conv2d-99          [-1, 256, 20, 30]         589,824\n     BatchNorm2d-100          [-1, 256, 20, 30]             512\n            ReLU-101          [-1, 256, 20, 30]               0\n      BasicBlock-102          [-1, 256, 20, 30]               0\n          Conv2d-103          [-1, 128, 20, 30]          32,896\n          Conv2d-104          [-1, 512, 10, 15]       1,179,648\n     BatchNorm2d-105          [-1, 512, 10, 15]           1,024\n            ReLU-106          [-1, 512, 10, 15]               0\n          Conv2d-107          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-108          [-1, 512, 10, 15]           1,024\n          Conv2d-109          [-1, 512, 10, 15]         131,072\n     BatchNorm2d-110          [-1, 512, 10, 15]           1,024\n            ReLU-111          [-1, 512, 10, 15]               0\n      BasicBlock-112          [-1, 512, 10, 15]               0\n          Conv2d-113          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-114          [-1, 512, 10, 15]           1,024\n            ReLU-115          [-1, 512, 10, 15]               0\n          Conv2d-116          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-117          [-1, 512, 10, 15]           1,024\n            ReLU-118          [-1, 512, 10, 15]               0\n      BasicBlock-119          [-1, 512, 10, 15]               0\n          Conv2d-120          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-121          [-1, 512, 10, 15]           1,024\n            ReLU-122          [-1, 512, 10, 15]               0\n          Conv2d-123          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-124          [-1, 512, 10, 15]           1,024\n            ReLU-125          [-1, 512, 10, 15]               0\n      BasicBlock-126          [-1, 512, 10, 15]               0\n          Conv2d-127          [-1, 512, 10, 15]         262,656\n ConvTranspose2d-128          [-1, 128, 20, 30]         262,272\n            ReLU-129          [-1, 256, 20, 30]               0\n     BatchNorm2d-130          [-1, 256, 20, 30]             512\nAdaptiveAvgPool2d-131            [-1, 256, 1, 1]               0\n          Linear-132                  [-1, 128]          32,896\n            ReLU-133                  [-1, 128]               0\n          Linear-134                  [-1, 256]          33,024\n         Sigmoid-135                  [-1, 256]               0\n        CSEBlock-136          [-1, 256, 20, 30]               0\n          Conv2d-137            [-1, 1, 20, 30]             257\n         Sigmoid-138            [-1, 1, 20, 30]               0\n        SSEBlock-139          [-1, 256, 20, 30]               0\n       SCSEBlock-140          [-1, 256, 20, 30]               0\n     UpsampBlock-141          [-1, 256, 20, 30]               0\n ConvTranspose2d-142          [-1, 128, 40, 60]         131,200\n            ReLU-143          [-1, 256, 40, 60]               0\n     BatchNorm2d-144          [-1, 256, 40, 60]             512\nAdaptiveAvgPool2d-145            [-1, 256, 1, 1]               0\n          Linear-146                  [-1, 128]          32,896\n            ReLU-147                  [-1, 128]               0\n          Linear-148                  [-1, 256]          33,024\n         Sigmoid-149                  [-1, 256]               0\n        CSEBlock-150          [-1, 256, 40, 60]               0\n          Conv2d-151            [-1, 1, 40, 60]             257\n         Sigmoid-152            [-1, 1, 40, 60]               0\n        SSEBlock-153          [-1, 256, 40, 60]               0\n       SCSEBlock-154          [-1, 256, 40, 60]               0\n     UpsampBlock-155          [-1, 256, 40, 60]               0\n ConvTranspose2d-156         [-1, 128, 80, 120]         131,200\n            ReLU-157         [-1, 256, 80, 120]               0\n     BatchNorm2d-158         [-1, 256, 80, 120]             512\nAdaptiveAvgPool2d-159            [-1, 256, 1, 1]               0\n          Linear-160                  [-1, 128]          32,896\n            ReLU-161                  [-1, 128]               0\n          Linear-162                  [-1, 256]          33,024\n         Sigmoid-163                  [-1, 256]               0\n        CSEBlock-164         [-1, 256, 80, 120]               0\n          Conv2d-165           [-1, 1, 80, 120]             257\n         Sigmoid-166           [-1, 1, 80, 120]               0\n        SSEBlock-167         [-1, 256, 80, 120]               0\n       SCSEBlock-168         [-1, 256, 80, 120]               0\n     UpsampBlock-169         [-1, 256, 80, 120]               0\n ConvTranspose2d-170        [-1, 128, 160, 240]         131,200\n            ReLU-171        [-1, 256, 160, 240]               0\n     BatchNorm2d-172        [-1, 256, 160, 240]             512\nAdaptiveAvgPool2d-173            [-1, 256, 1, 1]               0\n          Linear-174                  [-1, 128]          32,896\n            ReLU-175                  [-1, 128]               0\n          Linear-176                  [-1, 256]          33,024\n         Sigmoid-177                  [-1, 256]               0\n        CSEBlock-178        [-1, 256, 160, 240]               0\n          Conv2d-179          [-1, 1, 160, 240]             257\n         Sigmoid-180          [-1, 1, 160, 240]               0\n        SSEBlock-181        [-1, 256, 160, 240]               0\n       SCSEBlock-182        [-1, 256, 160, 240]               0\n     UpsampBlock-183        [-1, 256, 160, 240]               0\n ConvTranspose2d-184          [-1, 1, 320, 480]           1,025\n================================================================\nTotal params: 22,537,029\nTrainable params: 22,537,029\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.76\nForward/backward pass size (MB): 994.56\nParams size (MB): 85.97\nEstimated Total Size (MB): 1082.29\n----------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-ee16a739-e3a0-4689-a9ed-33c171733c59",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "10721a3d",
        "execution_millis": 951,
        "output_cleared": false,
        "execution_start": 1616403565085,
        "deepnote_cell_type": "code"
      },
      "source": "# from torchviz import make_dot\n\n# x = torch.randn(1,3, 320, 480)\n# y = net(x)\n\n# make_dot(y).view()",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'xdg-open'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-b37c17078a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/venv/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mview\u001b[0;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mto\u001b[0m \u001b[0mretrieve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mapplication\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mexit\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m--> 282\u001b[0;31m         return self.render(filename=filename, directory=directory,\n\u001b[0m\u001b[1;32m    283\u001b[0m                            \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                            quiet=quiet, quiet_view=quiet_view)\n",
            "\u001b[0;32m~/venv/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquiet_view\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/venv/lib/python3.8/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36m_view\u001b[0;34m(self, filepath, format, quiet)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                ' on %r platform' % (self.__class__, format,\n\u001b[1;32m    299\u001b[0m                                                     backend.PLATFORM))\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mview_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0m_view_darwin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdarwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/venv/lib/python3.8/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mview_unixoid\u001b[0;34m(filepath, quiet)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'view: %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mpopen_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen_stderr_devnull\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mquiet\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mpopen_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'xdg-open'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=411d58e9-cb4b-4924-bef0-2f383eff0187' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "96775278-d606-4bc7-a96c-64fcdaf37c69",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}