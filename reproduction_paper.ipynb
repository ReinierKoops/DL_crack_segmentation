{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reproduce DL\n## Automated Pavement Crack Segmentation",
      "metadata": {
        "tags": [],
        "cell_id": "00000-82d331e0-79e4-40f0-ba43-0ad2ccc7fea5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-0be6212f-ba68-4778-be63-87a42300ab47",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c1ecaa5f",
        "execution_millis": 1674,
        "execution_start": 1615984664280,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn as nn\nfrom torchsummary import summary\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Create a Residual block\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResBlock, self).__init__()\n        self.stride_one = (stride == 1)\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.conv_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        self.bn_shortcut = nn.BatchNorm2d(out_channels)\n\n        self.relu = torch.nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu(y)\n\n        y = self.conv2(y)\n        y = self.bn2(y)\n\n        if not self.stride_one:\n            x = self.bn_shortcut(self.conv_shortcut(x))\n        y += x\n\n        return self.relu(y)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # 3 input image channels, 64 output channels, 7x7 convolution, stride 2\n        # -- Blue --\n        self.conv1 = nn.Conv2d(3, 64, 7, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU()\n\n        # -- Green --\n        # maxpooling stride default is kernel size\n        self.maxpool_1_to_rs1 = nn.MaxPool2d(3, stride=2)\n\n        # Residual block 1\n        self.conv_rs1_1 = ResBlock(64, 64, 1)\n        self.conv_rs1_2 = ResBlock(64, 64, 1)\n        self.conv_rs1_3 = ResBlock(64, 64, 1)\n\n        # Residual block 2\n        self.conv_rs2_1 = ResBlock(64, 128, 2)\n        self.conv_rs2_2 = ResBlock(128, 128, 1)\n        self.conv_rs2_3 = ResBlock(128, 128, 1)\n        self.conv_rs2_4 = ResBlock(128, 128, 1)\n\n        # Residual block 3\n        self.conv_rs3_1 = ResBlock(128, 256, 2)\n        self.conv_rs3_2 = ResBlock(256, 256, 1)\n        self.conv_rs3_3 = ResBlock(256, 256, 1)\n        self.conv_rs3_4 = ResBlock(256, 256, 1)\n        self.conv_rs3_5 = ResBlock(256, 256, 1)\n        self.conv_rs3_6 = ResBlock(256, 256, 1)\n\n        # Residual block 4\n        self.conv_rs4_1 = ResBlock(256, 512, 2)\n        self.conv_rs4_2 = ResBlock(512, 512, 1)\n        self.conv_rs4_3 = ResBlock(512, 512, 1)\n\n        # -- Yellow --\n        # 64/128 input image channels, 128 output channels, 1x1 convolution, stride 1\n        # Not sure if stride should be 1...\n        # --\n        # Green (residual) block to Yellow block\n        self.conv_gr_to_yel_1 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_2 = nn.Conv2d(64, 128, 1, stride=1)\n        self.conv_gr_to_yel_3 = nn.Conv2d(128, 128, 1, stride=1)\n        self.conv_gr_to_yel_4 = nn.Conv2d(128, 128, 1, stride=1)\n        # Green (residual) block to Purple block\n        self.conv_gr_to_purp = nn.Conv2d(512, 512, 1, stride=1)\n\n    def forward(self, x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu1(y)\n        # y_y1 = self.conv_gr_to_yel_1(y)\n\n        y = self.maxpool_1_to_rs1(y)\n        y = self.conv_rs1_1(y)\n        y = self.conv_rs1_2(y)\n        y = self.conv_rs1_3(y)\n        # y_y2 = self.conv_gr_to_yel_2(y)\n\n        y = self.conv_rs2_1(y)\n        y = self.conv_rs2_2(y)\n        y = self.conv_rs2_3(y)\n        y = self.conv_rs2_4(y)\n        # y_y3 = self.conv_gr_to_yel_3(y)\n\n        y = self.conv_rs3_1(y)\n        y = self.conv_rs3_2(y)\n        y = self.conv_rs3_3(y)\n        y = self.conv_rs3_4(y)\n        y = self.conv_rs3_5(y)\n        y = self.conv_rs3_6(y)\n        # y_y4 = self.conv_gr_to_yel_4(y)\n\n        y = self.conv_rs4_1(y)\n        y = self.conv_rs4_2(y)\n        y = self.conv_rs4_3(y)\n        # y_y5 = self.conv_gr_to_purp(y)\n\n        return y\n\n\nnet = Net()\n# print(net)\nsummary(net, input_size=(3, 320, 480))",
      "outputs": [
        {
          "name": "stderr",
          "text": "/root/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n  return torch._C._cuda_getDeviceCount() > 0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 157, 237]           9,472\n       BatchNorm2d-2         [-1, 64, 157, 237]             128\n              ReLU-3         [-1, 64, 157, 237]               0\n         MaxPool2d-4          [-1, 64, 78, 118]               0\n            Conv2d-5          [-1, 64, 78, 118]          36,928\n       BatchNorm2d-6          [-1, 64, 78, 118]             128\n              ReLU-7          [-1, 64, 78, 118]               0\n            Conv2d-8          [-1, 64, 78, 118]          36,928\n       BatchNorm2d-9          [-1, 64, 78, 118]             128\n             ReLU-10          [-1, 64, 78, 118]               0\n         ResBlock-11          [-1, 64, 78, 118]               0\n           Conv2d-12          [-1, 64, 78, 118]          36,928\n      BatchNorm2d-13          [-1, 64, 78, 118]             128\n             ReLU-14          [-1, 64, 78, 118]               0\n           Conv2d-15          [-1, 64, 78, 118]          36,928\n      BatchNorm2d-16          [-1, 64, 78, 118]             128\n             ReLU-17          [-1, 64, 78, 118]               0\n         ResBlock-18          [-1, 64, 78, 118]               0\n           Conv2d-19          [-1, 64, 78, 118]          36,928\n      BatchNorm2d-20          [-1, 64, 78, 118]             128\n             ReLU-21          [-1, 64, 78, 118]               0\n           Conv2d-22          [-1, 64, 78, 118]          36,928\n      BatchNorm2d-23          [-1, 64, 78, 118]             128\n             ReLU-24          [-1, 64, 78, 118]               0\n         ResBlock-25          [-1, 64, 78, 118]               0\n           Conv2d-26          [-1, 128, 39, 59]          73,856\n      BatchNorm2d-27          [-1, 128, 39, 59]             256\n             ReLU-28          [-1, 128, 39, 59]               0\n           Conv2d-29          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-30          [-1, 128, 39, 59]             256\n           Conv2d-31          [-1, 128, 39, 59]           8,320\n      BatchNorm2d-32          [-1, 128, 39, 59]             256\n             ReLU-33          [-1, 128, 39, 59]               0\n         ResBlock-34          [-1, 128, 39, 59]               0\n           Conv2d-35          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-36          [-1, 128, 39, 59]             256\n             ReLU-37          [-1, 128, 39, 59]               0\n           Conv2d-38          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-39          [-1, 128, 39, 59]             256\n             ReLU-40          [-1, 128, 39, 59]               0\n         ResBlock-41          [-1, 128, 39, 59]               0\n           Conv2d-42          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-43          [-1, 128, 39, 59]             256\n             ReLU-44          [-1, 128, 39, 59]               0\n           Conv2d-45          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-46          [-1, 128, 39, 59]             256\n             ReLU-47          [-1, 128, 39, 59]               0\n         ResBlock-48          [-1, 128, 39, 59]               0\n           Conv2d-49          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-50          [-1, 128, 39, 59]             256\n             ReLU-51          [-1, 128, 39, 59]               0\n           Conv2d-52          [-1, 128, 39, 59]         147,584\n      BatchNorm2d-53          [-1, 128, 39, 59]             256\n             ReLU-54          [-1, 128, 39, 59]               0\n         ResBlock-55          [-1, 128, 39, 59]               0\n           Conv2d-56          [-1, 256, 20, 30]         295,168\n      BatchNorm2d-57          [-1, 256, 20, 30]             512\n             ReLU-58          [-1, 256, 20, 30]               0\n           Conv2d-59          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-60          [-1, 256, 20, 30]             512\n           Conv2d-61          [-1, 256, 20, 30]          33,024\n      BatchNorm2d-62          [-1, 256, 20, 30]             512\n             ReLU-63          [-1, 256, 20, 30]               0\n         ResBlock-64          [-1, 256, 20, 30]               0\n           Conv2d-65          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-66          [-1, 256, 20, 30]             512\n             ReLU-67          [-1, 256, 20, 30]               0\n           Conv2d-68          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-69          [-1, 256, 20, 30]             512\n             ReLU-70          [-1, 256, 20, 30]               0\n         ResBlock-71          [-1, 256, 20, 30]               0\n           Conv2d-72          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-73          [-1, 256, 20, 30]             512\n             ReLU-74          [-1, 256, 20, 30]               0\n           Conv2d-75          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-76          [-1, 256, 20, 30]             512\n             ReLU-77          [-1, 256, 20, 30]               0\n         ResBlock-78          [-1, 256, 20, 30]               0\n           Conv2d-79          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-80          [-1, 256, 20, 30]             512\n             ReLU-81          [-1, 256, 20, 30]               0\n           Conv2d-82          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-83          [-1, 256, 20, 30]             512\n             ReLU-84          [-1, 256, 20, 30]               0\n         ResBlock-85          [-1, 256, 20, 30]               0\n           Conv2d-86          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-87          [-1, 256, 20, 30]             512\n             ReLU-88          [-1, 256, 20, 30]               0\n           Conv2d-89          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-90          [-1, 256, 20, 30]             512\n             ReLU-91          [-1, 256, 20, 30]               0\n         ResBlock-92          [-1, 256, 20, 30]               0\n           Conv2d-93          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-94          [-1, 256, 20, 30]             512\n             ReLU-95          [-1, 256, 20, 30]               0\n           Conv2d-96          [-1, 256, 20, 30]         590,080\n      BatchNorm2d-97          [-1, 256, 20, 30]             512\n             ReLU-98          [-1, 256, 20, 30]               0\n         ResBlock-99          [-1, 256, 20, 30]               0\n          Conv2d-100          [-1, 512, 10, 15]       1,180,160\n     BatchNorm2d-101          [-1, 512, 10, 15]           1,024\n            ReLU-102          [-1, 512, 10, 15]               0\n          Conv2d-103          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-104          [-1, 512, 10, 15]           1,024\n          Conv2d-105          [-1, 512, 10, 15]         131,584\n     BatchNorm2d-106          [-1, 512, 10, 15]           1,024\n            ReLU-107          [-1, 512, 10, 15]               0\n        ResBlock-108          [-1, 512, 10, 15]               0\n          Conv2d-109          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-110          [-1, 512, 10, 15]           1,024\n            ReLU-111          [-1, 512, 10, 15]               0\n          Conv2d-112          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-113          [-1, 512, 10, 15]           1,024\n            ReLU-114          [-1, 512, 10, 15]               0\n        ResBlock-115          [-1, 512, 10, 15]               0\n          Conv2d-116          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-117          [-1, 512, 10, 15]           1,024\n            ReLU-118          [-1, 512, 10, 15]               0\n          Conv2d-119          [-1, 512, 10, 15]       2,359,808\n     BatchNorm2d-120          [-1, 512, 10, 15]           1,024\n            ReLU-121          [-1, 512, 10, 15]               0\n        ResBlock-122          [-1, 512, 10, 15]               0\n================================================================\nTotal params: 21,293,184\nTrainable params: 21,293,184\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.76\nForward/backward pass size (MB): 285.83\nParams size (MB): 81.23\nEstimated Total Size (MB): 368.81\n----------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=411d58e9-cb4b-4924-bef0-2f383eff0187' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "96775278-d606-4bc7-a96c-64fcdaf37c69",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}