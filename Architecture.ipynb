{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reproduce DL\n## Automated Pavement Crack Segmentation",
      "metadata": {
        "tags": [],
        "cell_id": "00000-7cace757-85ca-4fbe-8823-54ed93d2e0da",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d5972642",
        "execution_millis": 815,
        "cell_id": "00001-c80a09c0-d1a5-49cc-b53d-fa966bf35c4d",
        "execution_start": 1616668339387,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Residual block (Green)\n    ---\n    A special case of highway network without any gates\n    in their skip connection. Thus allowing the flow of\n    memory (or info) from initial layers to last layers.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride):\n        super(ResBlock, self).__init__()\n        self.stride_one = (stride == 1)\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        if not self.stride_one:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n                )\n\n    def forward(self, x):\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu(y)\n        y = self.conv2(y)\n        y = self.bn2(y)\n\n        if not self.stride_one:\n            x = self.downsample(x)\n        y += x\n\n        return self.relu(y)\n\n\nclass CSEBlock(nn.Module):\n    \"\"\"\n    Spatial Squeeze and Channel Excitation block\n    ---\n    Channel-wise focus\n\n    Recalibrates the channels by incorporating global\n    spatial information. It provides a receptive field\n    of whole spatial extent at the fc's.\n\n    Assign each channel (feature) of a convolutional \n    block (feature map) a different weightage \n    (excitation) based on how important each channel \n    is (squeeze) instead of equally weighing each\n    feature. This improves channel interdependencies.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(CSEBlock, self).__init__()\n        # Global pooling == AdaptiveAvgPool2d\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, _, _ = x.size()\n\n        avg_pool_x = self.avg_pool(x).view(batch_size, num_channels)\n\n        y = self.fc1(avg_pool_x)\n        y = self.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, num_channels, 1, 1)\n\n\nclass SSEBlock(nn.Module):\n    \"\"\"\n    Channel Squeeze and Spatial Excitation block\n    ---\n    Spatial-wise focus\n\n    It behaves like a spatial attention map indicating\n    where the network should focus more to aid the\n    segmentation.\n\n    Assign importance to spatial locations sort of \n    telling where features are better to focus\n    instead of reweighing which features are more\n    important.\n    \"\"\"\n\n    def __init__(self, in_channels):\n        super(SSEBlock, self).__init__()\n        # Output channel = 1, 1x1 convolution\n        self.conv = nn.Conv2d(in_channels, 1, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, H, W = x.size()\n\n        y = self.conv(x)\n        y = self.sigmoid(y)\n\n        return x * y.view(batch_size, 1, H, W)\n\n\nclass SCSEBlock(nn.Module):\n    \"\"\"\n    Spatial and Channel Squeeze and Excitation block\n    ---\n    Return the block with the most promising values.\n    \"\"\"\n\n    def __init__(self, in_channels, reduction=2):\n        super(SCSEBlock, self).__init__()\n        self.CSE = CSEBlock(in_channels, reduction)\n        self.SSE = SSEBlock(in_channels)\n\n    def forward(self, x):\n        return torch.max(self.CSE(x), self.SSE(x))\n\n\nclass UpsampBlock(nn.Module):\n    \"\"\"\n    Upsampling block\n    ---\n    Includes SCSEBlock\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super(UpsampBlock, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.scse = SCSEBlock(in_channels)\n\n    def forward(self, x):\n        y = self.relu(x)\n        y = self.bn(y)\n\n        return self.scse(y)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # 3 input image channels, 64 output channels, 7x7 convolution, stride 2, padding 3\n        # -- Blue --\n        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        # -- Green --\n        # maxpooling stride default is kernel size\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1, dilation=1, ceil_mode=False)\n\n        # Residual block 1\n        self.layer1 = nn.Sequential(\n            ResBlock(64, 64, 1), \n            ResBlock(64, 64, 1), \n            ResBlock(64, 64, 1)\n            )\n\n        # Residual block 2\n        self.layer2 = nn.Sequential(\n            ResBlock(64, 128, 2),\n            ResBlock(128, 128, 1),\n            ResBlock(128, 128, 1),\n            ResBlock(128, 128, 1)\n            )\n\n        # Residual block 3\n        self.layer3 = nn.Sequential(\n            ResBlock(128, 256, 2),\n            ResBlock(256, 256, 1),\n            ResBlock(256, 256, 1),\n            ResBlock(256, 256, 1),\n            ResBlock(256, 256, 1),\n            ResBlock(256, 256, 1)\n            )\n\n        # Residual block 4\n        self.layer4 = nn.Sequential(\n            ResBlock(256, 512, 2),\n            ResBlock(512, 512, 1),\n            ResBlock(512, 512, 1)\n            )\n\n        # # -- Yellow --\n        # # 64/128/256 input image channels, 128 output channels, 1x1 convolution, stride 1\n        # # Green (residual) block to Yellow block (Up to down)\n        # self.conv_gr_to_yel_1 = nn.Conv2d(64, 128, 1, stride=1)\n        # self.conv_gr_to_yel_2 = nn.Conv2d(64, 128, 1, stride=1)\n        # self.conv_gr_to_yel_3 = nn.Conv2d(128, 128, 1, stride=1)\n        # self.conv_gr_to_yel_4 = nn.Conv2d(256, 128, 1, stride=1)\n\n        # # -- Purple --\n        # # 512 input image channels, 512 output channels, 1x1 convolution, stride 1\n        # # Green (residual) block to Purple block\n        # self.conv_gr_to_purp = nn.Conv2d(512, 512, 1, stride=1)\n        # # Magenta block to Purple block\n        # self.conv_mag_to_purp_1 = UpsampBlock(256, 256)\n        # self.conv_mag_to_purp_2 = UpsampBlock(256, 256)\n        # self.conv_mag_to_purp_3 = UpsampBlock(256, 256)\n        # self.conv_mag_to_purp_4 = UpsampBlock(256, 256)\n\n        # # -- Magenta --\n        # # 512/256 input image channels, 128 output channels, 2x2 convolution, stride 2\n        # # Purple (residual) block to Magenta block (Down to up)\n        # self.conv_purp_to_mag_1 = nn.ConvTranspose2d(512, 128, 2, stride=2)\n        # self.conv_purp_to_mag_2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        # self.conv_purp_to_mag_3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        # self.conv_purp_to_mag_4 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        # self.conv_purp_to_mag_5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n\n    def forward(self, x):\n        # Traverse down the architecture (ResNet34)\n        y = self.conv1(x)\n        y = self.bn1(y)\n        y = self.relu1(y)\n        # y_y1 = self.conv_gr_to_yel_1(y)\n\n        y = self.maxpool(y)\n        y = self.layer1(y)\n        # y_y2 = self.conv_gr_to_yel_2(y)\n\n        y = self.layer2(y)\n        # y_y3 = self.conv_gr_to_yel_3(y)\n\n        y = self.layer3(y)\n        # y_y4 = self.conv_gr_to_yel_4(y)\n\n        y = self.layer4(y)\n        # y_y5 = self.conv_gr_to_purp(y)\n\n        # # Traverse up the U-based architecture\n        # y_y5 = self.conv_purp_to_mag_1(y_y5)\n\n        # y_y4 = torch.cat((y_y4, y_y5), dim=1)\n        # y_y4 = self.conv_mag_to_purp_1(y_y4)\n        # y_y4 = self.conv_purp_to_mag_2(y_y4)\n\n        # y_y3 = torch.cat((y_y3, y_y4), dim=1)\n        # y_y3 = self.conv_mag_to_purp_2(y_y3)\n        # y_y3 = self.conv_purp_to_mag_3(y_y3)\n\n        # y_y2 = torch.cat((y_y2, y_y3), dim=1)\n        # y_y2 = self.conv_mag_to_purp_3(y_y2)\n        # y_y2 = self.conv_purp_to_mag_4(y_y2)\n\n        # y_y1 = torch.cat((y_y1, y_y2), dim=1)\n        # y_y1 = self.conv_mag_to_purp_4(y_y1)\n        # return self.conv_purp_to_mag_5(y_y1)\n\n\nnet = Net()\n# print(net)\nsummary(net, input_size=(3, 320, 480))",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 160, 240]           9,408\n       BatchNorm2d-2         [-1, 64, 160, 240]             128\n              ReLU-3         [-1, 64, 160, 240]               0\n         MaxPool2d-4          [-1, 64, 80, 120]               0\n            Conv2d-5          [-1, 64, 80, 120]          36,864\n       BatchNorm2d-6          [-1, 64, 80, 120]             128\n              ReLU-7          [-1, 64, 80, 120]               0\n            Conv2d-8          [-1, 64, 80, 120]          36,864\n       BatchNorm2d-9          [-1, 64, 80, 120]             128\n             ReLU-10          [-1, 64, 80, 120]               0\n         ResBlock-11          [-1, 64, 80, 120]               0\n           Conv2d-12          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-13          [-1, 64, 80, 120]             128\n             ReLU-14          [-1, 64, 80, 120]               0\n           Conv2d-15          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-16          [-1, 64, 80, 120]             128\n             ReLU-17          [-1, 64, 80, 120]               0\n         ResBlock-18          [-1, 64, 80, 120]               0\n           Conv2d-19          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-20          [-1, 64, 80, 120]             128\n             ReLU-21          [-1, 64, 80, 120]               0\n           Conv2d-22          [-1, 64, 80, 120]          36,864\n      BatchNorm2d-23          [-1, 64, 80, 120]             128\n             ReLU-24          [-1, 64, 80, 120]               0\n         ResBlock-25          [-1, 64, 80, 120]               0\n           Conv2d-26          [-1, 128, 40, 60]          73,728\n      BatchNorm2d-27          [-1, 128, 40, 60]             256\n             ReLU-28          [-1, 128, 40, 60]               0\n           Conv2d-29          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-30          [-1, 128, 40, 60]             256\n           Conv2d-31          [-1, 128, 40, 60]           8,192\n      BatchNorm2d-32          [-1, 128, 40, 60]             256\n             ReLU-33          [-1, 128, 40, 60]               0\n         ResBlock-34          [-1, 128, 40, 60]               0\n           Conv2d-35          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-36          [-1, 128, 40, 60]             256\n             ReLU-37          [-1, 128, 40, 60]               0\n           Conv2d-38          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-39          [-1, 128, 40, 60]             256\n             ReLU-40          [-1, 128, 40, 60]               0\n         ResBlock-41          [-1, 128, 40, 60]               0\n           Conv2d-42          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-43          [-1, 128, 40, 60]             256\n             ReLU-44          [-1, 128, 40, 60]               0\n           Conv2d-45          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-46          [-1, 128, 40, 60]             256\n             ReLU-47          [-1, 128, 40, 60]               0\n         ResBlock-48          [-1, 128, 40, 60]               0\n           Conv2d-49          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-50          [-1, 128, 40, 60]             256\n             ReLU-51          [-1, 128, 40, 60]               0\n           Conv2d-52          [-1, 128, 40, 60]         147,456\n      BatchNorm2d-53          [-1, 128, 40, 60]             256\n             ReLU-54          [-1, 128, 40, 60]               0\n         ResBlock-55          [-1, 128, 40, 60]               0\n           Conv2d-56          [-1, 256, 20, 30]         294,912\n      BatchNorm2d-57          [-1, 256, 20, 30]             512\n             ReLU-58          [-1, 256, 20, 30]               0\n           Conv2d-59          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-60          [-1, 256, 20, 30]             512\n           Conv2d-61          [-1, 256, 20, 30]          32,768\n      BatchNorm2d-62          [-1, 256, 20, 30]             512\n             ReLU-63          [-1, 256, 20, 30]               0\n         ResBlock-64          [-1, 256, 20, 30]               0\n           Conv2d-65          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-66          [-1, 256, 20, 30]             512\n             ReLU-67          [-1, 256, 20, 30]               0\n           Conv2d-68          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-69          [-1, 256, 20, 30]             512\n             ReLU-70          [-1, 256, 20, 30]               0\n         ResBlock-71          [-1, 256, 20, 30]               0\n           Conv2d-72          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-73          [-1, 256, 20, 30]             512\n             ReLU-74          [-1, 256, 20, 30]               0\n           Conv2d-75          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-76          [-1, 256, 20, 30]             512\n             ReLU-77          [-1, 256, 20, 30]               0\n         ResBlock-78          [-1, 256, 20, 30]               0\n           Conv2d-79          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-80          [-1, 256, 20, 30]             512\n             ReLU-81          [-1, 256, 20, 30]               0\n           Conv2d-82          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-83          [-1, 256, 20, 30]             512\n             ReLU-84          [-1, 256, 20, 30]               0\n         ResBlock-85          [-1, 256, 20, 30]               0\n           Conv2d-86          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-87          [-1, 256, 20, 30]             512\n             ReLU-88          [-1, 256, 20, 30]               0\n           Conv2d-89          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-90          [-1, 256, 20, 30]             512\n             ReLU-91          [-1, 256, 20, 30]               0\n         ResBlock-92          [-1, 256, 20, 30]               0\n           Conv2d-93          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-94          [-1, 256, 20, 30]             512\n             ReLU-95          [-1, 256, 20, 30]               0\n           Conv2d-96          [-1, 256, 20, 30]         589,824\n      BatchNorm2d-97          [-1, 256, 20, 30]             512\n             ReLU-98          [-1, 256, 20, 30]               0\n         ResBlock-99          [-1, 256, 20, 30]               0\n          Conv2d-100          [-1, 512, 10, 15]       1,179,648\n     BatchNorm2d-101          [-1, 512, 10, 15]           1,024\n            ReLU-102          [-1, 512, 10, 15]               0\n          Conv2d-103          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-104          [-1, 512, 10, 15]           1,024\n          Conv2d-105          [-1, 512, 10, 15]         131,072\n     BatchNorm2d-106          [-1, 512, 10, 15]           1,024\n            ReLU-107          [-1, 512, 10, 15]               0\n        ResBlock-108          [-1, 512, 10, 15]               0\n          Conv2d-109          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-110          [-1, 512, 10, 15]           1,024\n            ReLU-111          [-1, 512, 10, 15]               0\n          Conv2d-112          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-113          [-1, 512, 10, 15]           1,024\n            ReLU-114          [-1, 512, 10, 15]               0\n        ResBlock-115          [-1, 512, 10, 15]               0\n          Conv2d-116          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-117          [-1, 512, 10, 15]           1,024\n            ReLU-118          [-1, 512, 10, 15]               0\n          Conv2d-119          [-1, 512, 10, 15]       2,359,296\n     BatchNorm2d-120          [-1, 512, 10, 15]           1,024\n            ReLU-121          [-1, 512, 10, 15]               0\n        ResBlock-122          [-1, 512, 10, 15]               0\n================================================================\nTotal params: 21,284,672\nTrainable params: 21,284,672\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.76\nForward/backward pass size (MB): 294.73\nParams size (MB): 81.19\nEstimated Total Size (MB): 377.68\n----------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-626cbe6c-3cfc-471b-833e-e272603873f7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7dd799d",
        "execution_millis": 540,
        "execution_start": 1616668342464,
        "deepnote_cell_type": "code"
      },
      "source": "import torchvision.models as models\nfrom collections import OrderedDict\n\n# Loading pre-trained model\npretrained_model = models.resnet34(pretrained=True)\n# Create new dict to remove last two layers.\nnew_pretrain_state_dict = OrderedDict()\n\nfor k, v in pretrained_model.state_dict().items():\n    if k != \"fc.weight\" and k != \"fc.bias\":\n        new_pretrain_state_dict[k] = v\n\nnet.load_state_dict(new_pretrain_state_dict)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 74,
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": null,
        "execution_millis": 951,
        "output_cleared": true,
        "execution_start": 1616403565085,
        "cell_id": "00003-8522b0a8-c7d6-48fd-95ec-473a41081a7f",
        "deepnote_cell_type": "code"
      },
      "source": "# from torchviz import make_dot\n\n# x = torch.randn(1,3, 320, 480)\n# y = net(x)\n\n# make_dot(y).view()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=411d58e9-cb4b-4924-bef0-2f383eff0187' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "33387df2-21dc-4075-93f9-705250ef69ba",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}